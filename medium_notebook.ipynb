{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets,transforms\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import pandas as pd\n",
    "\n",
    "import cv2\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST('classifier_data', train=True, download=True)\n",
    "test_dataset  = torchvision.datasets.MNIST('classifier_data', train=False, download=True)\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset.transform=transform\n",
    "test_dataset.transform=transform\n",
    "\n",
    "m=len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatenatedDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, annotations_file, img_dir, transform_positives=None, transform_background=None, target_transform=None\n",
    "    ):\n",
    "        self.img_labels = pd.read_csv(annotations_file, header=None)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform_positives = transform_positives  \n",
    "        self.transform_background = transform_background \n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        # image = Image.open(img_path)\n",
    "        image = cv2.imread(img_path)\n",
    "        \n",
    "        # By default OpenCV uses BGR color space for color images,\n",
    "        # so we need to convert the image to RGB color space.\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if label == 0 and self.transform_background:\n",
    "            image = self.transform_background(image=image)[\"image\"]\n",
    "        if label != 0 and self.transform_positives:\n",
    "            image = self.transform_positives(image=image)[\"image\"]\n",
    "        # if self.transform:\n",
    "        #     image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_HEIGHT=224\n",
    "IMAGE_WIDTH=224\n",
    "transform = A.Compose([\n",
    "        # A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "        A.RandomCrop(height=IMAGE_HEIGHT, width=IMAGE_WIDTH, always_apply=True),\n",
    "        A.Rotate(limit=35, p=1.0),\n",
    "        # A.ToGray(always_apply=True),\n",
    "\n",
    "        # A.Normalize(),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "whole_dataset = ConcatenatedDataset(annotations_file=\"imgs_merged/labels.csv\", img_dir=\"imgs_merged\", transform_background=transform, transform_positives=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self,h1=96):\n",
    "        # We optimize dropout rate in a convolutional neural network.\n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, stride=1, padding=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "        self.drop1=nn.Dropout2d(p=0.5)   \n",
    "\n",
    "        self.fc1 = nn.LazyLinear(h1)\n",
    "        self.drop2=nn.Dropout2d(p=0.1)\n",
    "\n",
    "        self.fc2 = nn.Linear(h1, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x),kernel_size = 2))\n",
    "\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x),kernel_size = 2))\n",
    "        x = self.drop1(x)\n",
    "\n",
    "        x = x.view(x.size(0),-1)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop2(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 1 input image channel (black & white), 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = torch.nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = torch.nn.Conv2d(6, 16, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = torch.nn.LazyLinear(120)  # 6*6 from image dimension\n",
    "        self.fc2 = torch.nn.Linear(120, 84)\n",
    "        self.fc3 = torch.nn.Linear(84, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "\n",
    "        x = x.view(x.size(0),-1)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "dataset = ConcatDataset([train_dataset, test_dataset])\n",
    "\n",
    "dataset = whole_dataset\n",
    "\n",
    "num_epochs=20\n",
    "batch_size=100\n",
    "k=10\n",
    "splits=KFold(n_splits=k,shuffle=True,random_state=42)\n",
    "foldperf={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model,device,dataloader,loss_fn,optimizer):\n",
    "    train_loss,train_correct=0.0,0\n",
    "    model.train()\n",
    "    for images, labels in dataloader:\n",
    "\n",
    "        images,labels = images.to(device),labels.to(device)\n",
    "        images = images.float()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(images)\n",
    "        # print(\"output:\", output.size())\n",
    "        # print(\"output:\", output)\n",
    "\n",
    "        # print(\"labels:\", labels.size())\n",
    "        # print(\"labels:\", labels.float())\n",
    "        \n",
    "        loss = loss_fn(output,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        scores, predictions = torch.max(output.data, 1)\n",
    "        train_correct += (predictions == labels).sum().item()\n",
    "\n",
    "    return train_loss,train_correct\n",
    "  \n",
    "def valid_epoch(model,device,dataloader,loss_fn):\n",
    "    valid_loss, val_correct = 0.0, 0\n",
    "    model.eval()\n",
    "    for images, labels in dataloader:\n",
    "\n",
    "        images,labels = images.to(device),labels.to(device)\n",
    "        images = images.float()\n",
    "        output = model(images)\n",
    "        loss=loss_fn(output,labels)\n",
    "        valid_loss+=loss.item()*images.size(0)\n",
    "        scores, predictions = torch.max(output.data,1)\n",
    "        val_correct+=(predictions == labels).sum().item()\n",
    "\n",
    "    return valid_loss,val_correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = whole_dataset.img_labels.reset_index()\n",
    "splitter = StratifiedKFold(n_splits=4, shuffle=True, random_state=0)\n",
    "\n",
    "splits = []\n",
    "for train_idx, test_idx in splitter.split(df[\"index\"], df[1]):\n",
    "    splits.append((train_idx, test_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piotr/Documents/studia/SEM6/TWM/road-markings-detection/.venv/lib/python3.9/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1/20 AVG Training Loss:3.450 AVG Test Loss:56.558 AVG Training Acc 22.22 % AVG Test Acc 33.33 %\n",
      "Epoch:2/20 AVG Training Loss:50.500 AVG Test Loss:126.293 AVG Training Acc 31.75 % AVG Test Acc 42.86 %\n",
      "Epoch:3/20 AVG Training Loss:119.114 AVG Test Loss:81.853 AVG Training Acc 46.03 % AVG Test Acc 42.86 %\n",
      "Epoch:4/20 AVG Training Loss:72.547 AVG Test Loss:12.893 AVG Training Acc 46.03 % AVG Test Acc 42.86 %\n",
      "Epoch:5/20 AVG Training Loss:12.607 AVG Test Loss:3.296 AVG Training Acc 46.03 % AVG Test Acc 23.81 %\n",
      "Epoch:6/20 AVG Training Loss:3.343 AVG Test Loss:1.905 AVG Training Acc 22.22 % AVG Test Acc 47.62 %\n",
      "Epoch:7/20 AVG Training Loss:1.985 AVG Test Loss:1.262 AVG Training Acc 44.44 % AVG Test Acc 33.33 %\n",
      "Epoch:8/20 AVG Training Loss:1.306 AVG Test Loss:1.319 AVG Training Acc 26.98 % AVG Test Acc 52.38 %\n",
      "Epoch:9/20 AVG Training Loss:1.292 AVG Test Loss:1.000 AVG Training Acc 55.56 % AVG Test Acc 76.19 %\n",
      "Epoch:10/20 AVG Training Loss:1.062 AVG Test Loss:1.179 AVG Training Acc 69.84 % AVG Test Acc 47.62 %\n",
      "Epoch:11/20 AVG Training Loss:1.123 AVG Test Loss:1.043 AVG Training Acc 44.44 % AVG Test Acc 52.38 %\n",
      "Epoch:12/20 AVG Training Loss:1.075 AVG Test Loss:0.966 AVG Training Acc 46.03 % AVG Test Acc 57.14 %\n",
      "Epoch:13/20 AVG Training Loss:1.007 AVG Test Loss:0.985 AVG Training Acc 52.38 % AVG Test Acc 57.14 %\n",
      "Epoch:14/20 AVG Training Loss:1.007 AVG Test Loss:0.916 AVG Training Acc 50.79 % AVG Test Acc 23.81 %\n",
      "Epoch:15/20 AVG Training Loss:0.957 AVG Test Loss:0.812 AVG Training Acc 23.81 % AVG Test Acc 38.10 %\n",
      "Epoch:16/20 AVG Training Loss:0.902 AVG Test Loss:0.855 AVG Training Acc 25.40 % AVG Test Acc 38.10 %\n",
      "Epoch:17/20 AVG Training Loss:0.834 AVG Test Loss:0.834 AVG Training Acc 39.68 % AVG Test Acc 42.86 %\n",
      "Epoch:18/20 AVG Training Loss:0.782 AVG Test Loss:0.699 AVG Training Acc 46.03 % AVG Test Acc 52.38 %\n",
      "Epoch:19/20 AVG Training Loss:0.669 AVG Test Loss:0.674 AVG Training Acc 53.97 % AVG Test Acc 57.14 %\n",
      "Epoch:20/20 AVG Training Loss:0.568 AVG Test Loss:0.558 AVG Training Acc 63.49 % AVG Test Acc 66.67 %\n",
      "Fold 2\n",
      "Epoch:1/20 AVG Training Loss:1.920 AVG Test Loss:26.148 AVG Training Acc 20.63 % AVG Test Acc 42.86 %\n",
      "Epoch:2/20 AVG Training Loss:23.756 AVG Test Loss:104.993 AVG Training Acc 46.03 % AVG Test Acc 33.33 %\n",
      "Epoch:3/20 AVG Training Loss:109.724 AVG Test Loss:27.586 AVG Training Acc 31.75 % AVG Test Acc 33.33 %\n",
      "Epoch:4/20 AVG Training Loss:24.659 AVG Test Loss:34.948 AVG Training Acc 31.75 % AVG Test Acc 66.67 %\n",
      "Epoch:5/20 AVG Training Loss:33.130 AVG Test Loss:20.026 AVG Training Acc 65.08 % AVG Test Acc 23.81 %\n",
      "Epoch:6/20 AVG Training Loss:20.429 AVG Test Loss:14.268 AVG Training Acc 22.22 % AVG Test Acc 23.81 %\n",
      "Epoch:7/20 AVG Training Loss:13.097 AVG Test Loss:3.533 AVG Training Acc 25.40 % AVG Test Acc 52.38 %\n",
      "Epoch:8/20 AVG Training Loss:3.468 AVG Test Loss:4.740 AVG Training Acc 52.38 % AVG Test Acc 71.43 %\n",
      "Epoch:9/20 AVG Training Loss:2.488 AVG Test Loss:5.550 AVG Training Acc 77.78 % AVG Test Acc 76.19 %\n",
      "Epoch:10/20 AVG Training Loss:3.939 AVG Test Loss:6.459 AVG Training Acc 77.78 % AVG Test Acc 76.19 %\n",
      "Epoch:11/20 AVG Training Loss:3.966 AVG Test Loss:3.962 AVG Training Acc 79.37 % AVG Test Acc 76.19 %\n",
      "Epoch:12/20 AVG Training Loss:2.529 AVG Test Loss:1.891 AVG Training Acc 77.78 % AVG Test Acc 80.95 %\n",
      "Epoch:13/20 AVG Training Loss:1.750 AVG Test Loss:1.060 AVG Training Acc 79.37 % AVG Test Acc 76.19 %\n",
      "Epoch:14/20 AVG Training Loss:0.743 AVG Test Loss:0.531 AVG Training Acc 88.89 % AVG Test Acc 71.43 %\n",
      "Epoch:15/20 AVG Training Loss:0.504 AVG Test Loss:0.646 AVG Training Acc 68.25 % AVG Test Acc 57.14 %\n",
      "Epoch:16/20 AVG Training Loss:0.544 AVG Test Loss:0.831 AVG Training Acc 63.49 % AVG Test Acc 42.86 %\n",
      "Epoch:17/20 AVG Training Loss:0.765 AVG Test Loss:1.110 AVG Training Acc 50.79 % AVG Test Acc 28.57 %\n",
      "Epoch:18/20 AVG Training Loss:0.982 AVG Test Loss:0.998 AVG Training Acc 41.27 % AVG Test Acc 33.33 %\n",
      "Epoch:19/20 AVG Training Loss:0.937 AVG Test Loss:0.938 AVG Training Acc 39.68 % AVG Test Acc 38.10 %\n",
      "Epoch:20/20 AVG Training Loss:0.721 AVG Test Loss:0.709 AVG Training Acc 49.21 % AVG Test Acc 47.62 %\n",
      "Fold 3\n",
      "Epoch:1/20 AVG Training Loss:1.132 AVG Test Loss:11.867 AVG Training Acc 26.98 % AVG Test Acc 47.62 %\n",
      "Epoch:2/20 AVG Training Loss:13.161 AVG Test Loss:0.918 AVG Training Acc 44.44 % AVG Test Acc 52.38 %\n",
      "Epoch:3/20 AVG Training Loss:0.970 AVG Test Loss:0.709 AVG Training Acc 44.44 % AVG Test Acc 57.14 %\n",
      "Epoch:4/20 AVG Training Loss:0.721 AVG Test Loss:0.369 AVG Training Acc 57.14 % AVG Test Acc 71.43 %\n",
      "Epoch:5/20 AVG Training Loss:0.571 AVG Test Loss:1.106 AVG Training Acc 60.32 % AVG Test Acc 85.71 %\n",
      "Epoch:6/20 AVG Training Loss:0.585 AVG Test Loss:0.429 AVG Training Acc 88.89 % AVG Test Acc 90.48 %\n",
      "Epoch:7/20 AVG Training Loss:0.696 AVG Test Loss:0.246 AVG Training Acc 90.48 % AVG Test Acc 95.24 %\n",
      "Epoch:8/20 AVG Training Loss:0.260 AVG Test Loss:0.336 AVG Training Acc 95.24 % AVG Test Acc 90.48 %\n",
      "Epoch:9/20 AVG Training Loss:0.267 AVG Test Loss:0.353 AVG Training Acc 92.06 % AVG Test Acc 85.71 %\n",
      "Epoch:10/20 AVG Training Loss:0.435 AVG Test Loss:0.073 AVG Training Acc 82.54 % AVG Test Acc 100.00 %\n",
      "Epoch:11/20 AVG Training Loss:0.112 AVG Test Loss:0.766 AVG Training Acc 96.83 % AVG Test Acc 95.24 %\n",
      "Epoch:12/20 AVG Training Loss:0.234 AVG Test Loss:0.140 AVG Training Acc 95.24 % AVG Test Acc 95.24 %\n",
      "Epoch:13/20 AVG Training Loss:0.237 AVG Test Loss:0.110 AVG Training Acc 93.65 % AVG Test Acc 95.24 %\n",
      "Epoch:14/20 AVG Training Loss:0.408 AVG Test Loss:0.175 AVG Training Acc 93.65 % AVG Test Acc 90.48 %\n",
      "Epoch:15/20 AVG Training Loss:0.167 AVG Test Loss:0.123 AVG Training Acc 93.65 % AVG Test Acc 95.24 %\n",
      "Epoch:16/20 AVG Training Loss:0.167 AVG Test Loss:0.028 AVG Training Acc 95.24 % AVG Test Acc 100.00 %\n",
      "Epoch:17/20 AVG Training Loss:0.149 AVG Test Loss:0.031 AVG Training Acc 93.65 % AVG Test Acc 100.00 %\n",
      "Epoch:18/20 AVG Training Loss:0.029 AVG Test Loss:0.022 AVG Training Acc 100.00 % AVG Test Acc 100.00 %\n",
      "Epoch:19/20 AVG Training Loss:0.105 AVG Test Loss:0.017 AVG Training Acc 98.41 % AVG Test Acc 100.00 %\n",
      "Epoch:20/20 AVG Training Loss:0.070 AVG Test Loss:0.016 AVG Training Acc 98.41 % AVG Test Acc 100.00 %\n",
      "Fold 4\n",
      "Epoch:1/20 AVG Training Loss:2.326 AVG Test Loss:140.384 AVG Training Acc 25.40 % AVG Test Acc 47.62 %\n",
      "Epoch:2/20 AVG Training Loss:146.513 AVG Test Loss:29.508 AVG Training Acc 44.44 % AVG Test Acc 33.33 %\n",
      "Epoch:3/20 AVG Training Loss:28.451 AVG Test Loss:13.356 AVG Training Acc 31.75 % AVG Test Acc 61.90 %\n",
      "Epoch:4/20 AVG Training Loss:15.641 AVG Test Loss:16.409 AVG Training Acc 52.38 % AVG Test Acc 19.05 %\n",
      "Epoch:5/20 AVG Training Loss:13.044 AVG Test Loss:0.986 AVG Training Acc 23.81 % AVG Test Acc 85.71 %\n",
      "Epoch:6/20 AVG Training Loss:0.966 AVG Test Loss:2.580 AVG Training Acc 82.54 % AVG Test Acc 85.71 %\n",
      "Epoch:7/20 AVG Training Loss:1.770 AVG Test Loss:3.499 AVG Training Acc 87.30 % AVG Test Acc 80.95 %\n",
      "Epoch:8/20 AVG Training Loss:2.984 AVG Test Loss:0.706 AVG Training Acc 80.95 % AVG Test Acc 85.71 %\n",
      "Epoch:9/20 AVG Training Loss:1.695 AVG Test Loss:2.082 AVG Training Acc 82.54 % AVG Test Acc 57.14 %\n",
      "Epoch:10/20 AVG Training Loss:1.865 AVG Test Loss:0.448 AVG Training Acc 49.21 % AVG Test Acc 85.71 %\n",
      "Epoch:11/20 AVG Training Loss:0.471 AVG Test Loss:1.149 AVG Training Acc 92.06 % AVG Test Acc 85.71 %\n",
      "Epoch:12/20 AVG Training Loss:0.975 AVG Test Loss:1.501 AVG Training Acc 85.71 % AVG Test Acc 80.95 %\n",
      "Epoch:13/20 AVG Training Loss:1.531 AVG Test Loss:0.008 AVG Training Acc 80.95 % AVG Test Acc 100.00 %\n",
      "Epoch:14/20 AVG Training Loss:0.985 AVG Test Loss:0.001 AVG Training Acc 85.71 % AVG Test Acc 100.00 %\n",
      "Epoch:15/20 AVG Training Loss:0.214 AVG Test Loss:0.225 AVG Training Acc 98.41 % AVG Test Acc 95.24 %\n",
      "Epoch:16/20 AVG Training Loss:0.287 AVG Test Loss:0.383 AVG Training Acc 87.30 % AVG Test Acc 90.48 %\n",
      "Epoch:17/20 AVG Training Loss:0.120 AVG Test Loss:0.269 AVG Training Acc 95.24 % AVG Test Acc 95.24 %\n",
      "Epoch:18/20 AVG Training Loss:0.008 AVG Test Loss:0.003 AVG Training Acc 100.00 % AVG Test Acc 100.00 %\n",
      "Epoch:19/20 AVG Training Loss:0.006 AVG Test Loss:0.031 AVG Training Acc 100.00 % AVG Test Acc 100.00 %\n",
      "Epoch:20/20 AVG Training Loss:0.102 AVG Test Loss:0.023 AVG Training Acc 98.41 % AVG Test Acc 100.00 %\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_idx, val_idx) in enumerate(splits):\n",
    "\n",
    "    # print(train_idx, val_idx)\n",
    "    print(\"Fold {}\".format(fold + 1))\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(val_idx)\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "    test_loader = DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = ConvNet()\n",
    "    model = LeNet()\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.002)\n",
    "\n",
    "    history = {\"train_loss\": [], \"test_loss\": [], \"train_acc\": [], \"test_acc\": []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_correct = train_epoch(\n",
    "            model, device, train_loader, criterion, optimizer\n",
    "        )\n",
    "        test_loss, test_correct = valid_epoch(model, device, test_loader, criterion)\n",
    "\n",
    "        train_loss = train_loss / len(train_loader.sampler)\n",
    "        train_acc = train_correct / len(train_loader.sampler) * 100\n",
    "        test_loss = test_loss / len(test_loader.sampler)\n",
    "        test_acc = test_correct / len(test_loader.sampler) * 100\n",
    "\n",
    "        print(\n",
    "            \"Epoch:{}/{} AVG Training Loss:{:.3f} AVG Test Loss:{:.3f} AVG Training Acc {:.2f} % AVG Test Acc {:.2f} %\".format(\n",
    "                epoch + 1, num_epochs, train_loss, test_loss, train_acc, test_acc\n",
    "            )\n",
    "        )\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"test_loss\"].append(test_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    foldperf[\"fold{}\".format(fold + 1)] = history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of 4 fold cross validation\n",
      "Average Training Loss: 9.583 \t Average Test Loss: 9.853 \t Average Training Acc: 63.39 \t Average Test Acc: 66.49\n"
     ]
    }
   ],
   "source": [
    "testl_f,tl_f,testa_f,ta_f=[],[],[],[]\n",
    "k=4\n",
    "for f in range(1,k+1):\n",
    "\n",
    "     tl_f.append(np.mean(foldperf['fold{}'.format(f)]['train_loss']))\n",
    "     testl_f.append(np.mean(foldperf['fold{}'.format(f)]['test_loss']))\n",
    "\n",
    "     ta_f.append(np.mean(foldperf['fold{}'.format(f)]['train_acc']))\n",
    "     testa_f.append(np.mean(foldperf['fold{}'.format(f)]['test_acc']))\n",
    "\n",
    "print('Performance of {} fold cross validation'.format(k))\n",
    "print(\"Average Training Loss: {:.3f} \\t Average Test Loss: {:.3f} \\t Average Training Acc: {:.2f} \\t Average Test Acc: {:.2f}\".format(np.mean(tl_f),np.mean(testl_f),np.mean(ta_f),np.mean(testa_f)))     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "92817c6158c1804639fa34720e1636a07923a028e1d05ddaa4c830e80be69f5c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
