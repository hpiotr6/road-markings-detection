{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets,transforms\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import pandas as pd\n",
    "\n",
    "import cv2\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST('classifier_data', train=True, download=True)\n",
    "test_dataset  = torchvision.datasets.MNIST('classifier_data', train=False, download=True)\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset.transform=transform\n",
    "test_dataset.transform=transform\n",
    "\n",
    "m=len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatenatedDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, annotations_file, img_dir, transform_positives=None, transform_background=None, target_transform=None\n",
    "    ):\n",
    "        self.img_labels = pd.read_csv(annotations_file, header=None)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform_positives = transform_positives  \n",
    "        self.transform_background = transform_background \n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        # image = Image.open(img_path)\n",
    "        image = cv2.imread(img_path)\n",
    "        \n",
    "        # By default OpenCV uses BGR color space for color images,\n",
    "        # so we need to convert the image to RGB color space.\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if label == 0 and self.transform_background:\n",
    "            image = self.transform_background(image=image)[\"image\"]\n",
    "        if label != 0 and self.transform_positives:\n",
    "            image = self.transform_positives(image=image)[\"image\"]\n",
    "        # if self.transform:\n",
    "        #     image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_HEIGHT=224\n",
    "IMAGE_WIDTH=224\n",
    "transform = A.Compose([\n",
    "        # A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "        A.RandomCrop(height=IMAGE_HEIGHT, width=IMAGE_WIDTH, always_apply=True),\n",
    "        A.Rotate(limit=35, p=1.0),\n",
    "        # A.ToGray(always_apply=True),\n",
    "\n",
    "        # A.Normalize(),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "whole_dataset = ConcatenatedDataset(annotations_file=\"imgs_merged/labels.csv\", img_dir=\"imgs_merged\", transform_background=transform, transform_positives=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self,h1=96):\n",
    "        # We optimize dropout rate in a convolutional neural network.\n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, stride=1, padding=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "        self.drop1=nn.Dropout2d(p=0.5)   \n",
    "\n",
    "        self.fc1 = nn.LazyLinear(h1)\n",
    "        self.drop2=nn.Dropout2d(p=0.1)\n",
    "\n",
    "        self.fc2 = nn.Linear(h1, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x),kernel_size = 2))\n",
    "\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x),kernel_size = 2))\n",
    "        x = self.drop1(x)\n",
    "\n",
    "        x = x.view(x.size(0),-1)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop2(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 1 input image channel (black & white), 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = torch.nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = torch.nn.Conv2d(6, 16, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = torch.nn.LazyLinear(120)  # 6*6 from image dimension\n",
    "        self.fc2 = torch.nn.Linear(120, 84)\n",
    "        self.fc3 = torch.nn.Linear(84, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "\n",
    "        x = x.view(x.size(0),-1)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "dataset = ConcatDataset([train_dataset, test_dataset])\n",
    "\n",
    "dataset = whole_dataset\n",
    "\n",
    "num_epochs=20\n",
    "batch_size=100\n",
    "k=10\n",
    "splits=KFold(n_splits=k,shuffle=True,random_state=42)\n",
    "foldperf={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model,device,dataloader,loss_fn,optimizer):\n",
    "    train_loss,train_correct=0.0,0\n",
    "    model.train()\n",
    "    for images, labels in dataloader:\n",
    "\n",
    "        images,labels = images.to(device),labels.to(device)\n",
    "        images = images.float()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(images)\n",
    "        # print(\"output:\", output.size())\n",
    "        # print(\"output:\", output)\n",
    "\n",
    "        # print(\"labels:\", labels.size())\n",
    "        # print(\"labels:\", labels.float())\n",
    "        \n",
    "        loss = loss_fn(output,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        scores, predictions = torch.max(output.data, 1)\n",
    "        train_correct += (predictions == labels).sum().item()\n",
    "\n",
    "    return train_loss,train_correct\n",
    "  \n",
    "def valid_epoch(model,device,dataloader,loss_fn):\n",
    "    valid_loss, val_correct = 0.0, 0\n",
    "    model.eval()\n",
    "    for images, labels in dataloader:\n",
    "\n",
    "        images,labels = images.to(device),labels.to(device)\n",
    "        images = images.float()\n",
    "        output = model(images)\n",
    "        loss=loss_fn(output,labels)\n",
    "        valid_loss+=loss.item()*images.size(0)\n",
    "        scores, predictions = torch.max(output.data,1)\n",
    "        val_correct+=(predictions == labels).sum().item()\n",
    "\n",
    "    return valid_loss,val_correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = whole_dataset.img_labels.reset_index()\n",
    "splitter = StratifiedKFold(n_splits=4, shuffle=True, random_state=0)\n",
    "\n",
    "splits = []\n",
    "for train_idx, test_idx in splitter.split(df[\"index\"], df[1]):\n",
    "    splits.append((train_idx, test_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piotr/Documents/studia/SEM6/TWM/road-markings-detection/.venv/lib/python3.9/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([63, 6, 110, 110])\n",
      "torch.Size([63, 16, 54, 54])\n",
      "torch.Size([63, 46656])\n",
      "torch.Size([63, 120])\n",
      "torch.Size([63, 84])\n",
      "torch.Size([63, 3])\n",
      "torch.Size([21, 6, 110, 110])\n",
      "torch.Size([21, 16, 54, 54])\n",
      "torch.Size([21, 46656])\n",
      "torch.Size([21, 120])\n",
      "torch.Size([21, 84])\n",
      "torch.Size([21, 3])\n",
      "Epoch:1/20 AVG Training Loss:3.524 AVG Test Loss:59.718 AVG Training Acc 22.22 % AVG Test Acc 33.33 %\n",
      "torch.Size([63, 6, 110, 110])\n",
      "torch.Size([63, 16, 54, 54])\n",
      "torch.Size([63, 46656])\n",
      "torch.Size([63, 120])\n",
      "torch.Size([63, 84])\n",
      "torch.Size([63, 3])\n",
      "torch.Size([21, 6, 110, 110])\n",
      "torch.Size([21, 16, 54, 54])\n",
      "torch.Size([21, 46656])\n",
      "torch.Size([21, 120])\n",
      "torch.Size([21, 84])\n",
      "torch.Size([21, 3])\n",
      "Epoch:2/20 AVG Training Loss:66.188 AVG Test Loss:82.814 AVG Training Acc 31.75 % AVG Test Acc 42.86 %\n",
      "torch.Size([63, 6, 110, 110])\n",
      "torch.Size([63, 16, 54, 54])\n",
      "torch.Size([63, 46656])\n",
      "torch.Size([63, 120])\n",
      "torch.Size([63, 84])\n",
      "torch.Size([63, 3])\n",
      "torch.Size([21, 6, 110, 110])\n",
      "torch.Size([21, 16, 54, 54])\n",
      "torch.Size([21, 46656])\n",
      "torch.Size([21, 120])\n",
      "torch.Size([21, 84])\n",
      "torch.Size([21, 3])\n",
      "Epoch:3/20 AVG Training Loss:88.780 AVG Test Loss:42.058 AVG Training Acc 46.03 % AVG Test Acc 42.86 %\n",
      "torch.Size([63, 6, 110, 110])\n",
      "torch.Size([63, 16, 54, 54])\n",
      "torch.Size([63, 46656])\n",
      "torch.Size([63, 120])\n",
      "torch.Size([63, 84])\n",
      "torch.Size([63, 3])\n",
      "torch.Size([21, 6, 110, 110])\n",
      "torch.Size([21, 16, 54, 54])\n",
      "torch.Size([21, 46656])\n",
      "torch.Size([21, 120])\n",
      "torch.Size([21, 84])\n",
      "torch.Size([21, 3])\n",
      "Epoch:4/20 AVG Training Loss:44.422 AVG Test Loss:7.507 AVG Training Acc 46.03 % AVG Test Acc 66.67 %\n",
      "torch.Size([63, 6, 110, 110])\n",
      "torch.Size([63, 16, 54, 54])\n",
      "torch.Size([63, 46656])\n",
      "torch.Size([63, 120])\n",
      "torch.Size([63, 84])\n",
      "torch.Size([63, 3])\n",
      "torch.Size([21, 6, 110, 110])\n",
      "torch.Size([21, 16, 54, 54])\n",
      "torch.Size([21, 46656])\n",
      "torch.Size([21, 120])\n",
      "torch.Size([21, 84])\n",
      "torch.Size([21, 3])\n",
      "Epoch:5/20 AVG Training Loss:7.050 AVG Test Loss:1.712 AVG Training Acc 61.90 % AVG Test Acc 23.81 %\n",
      "torch.Size([63, 6, 110, 110])\n",
      "torch.Size([63, 16, 54, 54])\n",
      "torch.Size([63, 46656])\n",
      "torch.Size([63, 120])\n",
      "torch.Size([63, 84])\n",
      "torch.Size([63, 3])\n",
      "torch.Size([21, 6, 110, 110])\n",
      "torch.Size([21, 16, 54, 54])\n",
      "torch.Size([21, 46656])\n",
      "torch.Size([21, 120])\n",
      "torch.Size([21, 84])\n",
      "torch.Size([21, 3])\n",
      "Epoch:6/20 AVG Training Loss:1.958 AVG Test Loss:0.783 AVG Training Acc 22.22 % AVG Test Acc 52.38 %\n",
      "torch.Size([63, 6, 110, 110])\n",
      "torch.Size([63, 16, 54, 54])\n",
      "torch.Size([63, 46656])\n",
      "torch.Size([63, 120])\n",
      "torch.Size([63, 84])\n",
      "torch.Size([63, 3])\n",
      "torch.Size([21, 6, 110, 110])\n",
      "torch.Size([21, 16, 54, 54])\n",
      "torch.Size([21, 46656])\n",
      "torch.Size([21, 120])\n",
      "torch.Size([21, 84])\n",
      "torch.Size([21, 3])\n",
      "Epoch:7/20 AVG Training Loss:0.841 AVG Test Loss:0.732 AVG Training Acc 44.44 % AVG Test Acc 61.90 %\n",
      "torch.Size([63, 6, 110, 110])\n",
      "torch.Size([63, 16, 54, 54])\n",
      "torch.Size([63, 46656])\n",
      "torch.Size([63, 120])\n",
      "torch.Size([63, 84])\n",
      "torch.Size([63, 3])\n",
      "torch.Size([21, 6, 110, 110])\n",
      "torch.Size([21, 16, 54, 54])\n",
      "torch.Size([21, 46656])\n",
      "torch.Size([21, 120])\n",
      "torch.Size([21, 84])\n",
      "torch.Size([21, 3])\n",
      "Epoch:8/20 AVG Training Loss:0.720 AVG Test Loss:0.876 AVG Training Acc 63.49 % AVG Test Acc 42.86 %\n",
      "torch.Size([63, 6, 110, 110])\n",
      "torch.Size([63, 16, 54, 54])\n",
      "torch.Size([63, 46656])\n",
      "torch.Size([63, 120])\n",
      "torch.Size([63, 84])\n",
      "torch.Size([63, 3])\n",
      "torch.Size([21, 6, 110, 110])\n",
      "torch.Size([21, 16, 54, 54])\n",
      "torch.Size([21, 46656])\n",
      "torch.Size([21, 120])\n",
      "torch.Size([21, 84])\n",
      "torch.Size([21, 3])\n",
      "Epoch:9/20 AVG Training Loss:0.867 AVG Test Loss:0.832 AVG Training Acc 55.56 % AVG Test Acc 42.86 %\n",
      "torch.Size([63, 6, 110, 110])\n",
      "torch.Size([63, 16, 54, 54])\n",
      "torch.Size([63, 46656])\n",
      "torch.Size([63, 120])\n",
      "torch.Size([63, 84])\n",
      "torch.Size([63, 3])\n",
      "torch.Size([21, 6, 110, 110])\n",
      "torch.Size([21, 16, 54, 54])\n",
      "torch.Size([21, 46656])\n",
      "torch.Size([21, 120])\n",
      "torch.Size([21, 84])\n",
      "torch.Size([21, 3])\n",
      "Epoch:10/20 AVG Training Loss:0.817 AVG Test Loss:0.928 AVG Training Acc 57.14 % AVG Test Acc 61.90 %\n",
      "torch.Size([63, 6, 110, 110])\n",
      "torch.Size([63, 16, 54, 54])\n",
      "torch.Size([63, 46656])\n",
      "torch.Size([63, 120])\n",
      "torch.Size([63, 84])\n",
      "torch.Size([63, 3])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [86]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m     21\u001b[0m     train_loss, train_correct \u001b[39m=\u001b[39m train_epoch(\n\u001b[1;32m     22\u001b[0m         model, device, train_loader, criterion, optimizer\n\u001b[1;32m     23\u001b[0m     )\n\u001b[0;32m---> 24\u001b[0m     test_loss, test_correct \u001b[39m=\u001b[39m valid_epoch(model, device, test_loader, criterion)\n\u001b[1;32m     26\u001b[0m     train_loss \u001b[39m=\u001b[39m train_loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(train_loader\u001b[39m.\u001b[39msampler)\n\u001b[1;32m     27\u001b[0m     train_acc \u001b[39m=\u001b[39m train_correct \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(train_loader\u001b[39m.\u001b[39msampler) \u001b[39m*\u001b[39m \u001b[39m100\u001b[39m\n",
      "\u001b[1;32m/Users/piotr/Documents/studia/SEM6/TWM/road-markings-detection/medium_notebook.ipynb Cell 8'\u001b[0m in \u001b[0;36mvalid_epoch\u001b[0;34m(model, device, dataloader, loss_fn)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/piotr/Documents/studia/SEM6/TWM/road-markings-detection/medium_notebook.ipynb#ch0000008?line=30'>31</a>\u001b[0m images,labels \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device),labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/piotr/Documents/studia/SEM6/TWM/road-markings-detection/medium_notebook.ipynb#ch0000008?line=31'>32</a>\u001b[0m images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/piotr/Documents/studia/SEM6/TWM/road-markings-detection/medium_notebook.ipynb#ch0000008?line=32'>33</a>\u001b[0m output \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/piotr/Documents/studia/SEM6/TWM/road-markings-detection/medium_notebook.ipynb#ch0000008?line=33'>34</a>\u001b[0m loss\u001b[39m=\u001b[39mloss_fn(output,labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/piotr/Documents/studia/SEM6/TWM/road-markings-detection/medium_notebook.ipynb#ch0000008?line=34'>35</a>\u001b[0m valid_loss\u001b[39m+\u001b[39m\u001b[39m=\u001b[39mloss\u001b[39m.\u001b[39mitem()\u001b[39m*\u001b[39mimages\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/studia/SEM6/TWM/road-markings-detection/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/piotr/Documents/studia/SEM6/TWM/road-markings-detection/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/piotr/Documents/studia/SEM6/TWM/road-markings-detection/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/piotr/Documents/studia/SEM6/TWM/road-markings-detection/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/piotr/Documents/studia/SEM6/TWM/road-markings-detection/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/piotr/Documents/studia/SEM6/TWM/road-markings-detection/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/piotr/Documents/studia/SEM6/TWM/road-markings-detection/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/piotr/Documents/studia/SEM6/TWM/road-markings-detection/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/piotr/Documents/studia/SEM6/TWM/road-markings-detection/medium_notebook.ipynb Cell 6'\u001b[0m in \u001b[0;36mLeNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/piotr/Documents/studia/SEM6/TWM/road-markings-detection/medium_notebook.ipynb#ch0000002?line=12'>13</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/piotr/Documents/studia/SEM6/TWM/road-markings-detection/medium_notebook.ipynb#ch0000002?line=13'>14</a>\u001b[0m     \u001b[39m# Max pooling over a (2, 2) window\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/piotr/Documents/studia/SEM6/TWM/road-markings-detection/medium_notebook.ipynb#ch0000002?line=14'>15</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmax_pool2d(F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)), (\u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/piotr/Documents/studia/SEM6/TWM/road-markings-detection/medium_notebook.ipynb#ch0000002?line=15'>16</a>\u001b[0m     \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/piotr/Documents/studia/SEM6/TWM/road-markings-detection/medium_notebook.ipynb#ch0000002?line=16'>17</a>\u001b[0m     \u001b[39m# If the size is a square you can only specify a single number\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/studia/SEM6/TWM/road-markings-detection/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/piotr/Documents/studia/SEM6/TWM/road-markings-detection/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/piotr/Documents/studia/SEM6/TWM/road-markings-detection/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/piotr/Documents/studia/SEM6/TWM/road-markings-detection/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/piotr/Documents/studia/SEM6/TWM/road-markings-detection/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/piotr/Documents/studia/SEM6/TWM/road-markings-detection/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/piotr/Documents/studia/SEM6/TWM/road-markings-detection/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/piotr/Documents/studia/SEM6/TWM/road-markings-detection/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/studia/SEM6/TWM/road-markings-detection/.venv/lib/python3.9/site-packages/torch/nn/modules/conv.py:447\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/piotr/Documents/studia/SEM6/TWM/road-markings-detection/.venv/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=445'>446</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> <a href='file:///Users/piotr/Documents/studia/SEM6/TWM/road-markings-detection/.venv/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=446'>447</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/Documents/studia/SEM6/TWM/road-markings-detection/.venv/lib/python3.9/site-packages/torch/nn/modules/conv.py:443\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/piotr/Documents/studia/SEM6/TWM/road-markings-detection/.venv/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=438'>439</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    <a href='file:///Users/piotr/Documents/studia/SEM6/TWM/road-markings-detection/.venv/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=439'>440</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    <a href='file:///Users/piotr/Documents/studia/SEM6/TWM/road-markings-detection/.venv/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=440'>441</a>\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    <a href='file:///Users/piotr/Documents/studia/SEM6/TWM/road-markings-detection/.venv/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=441'>442</a>\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> <a href='file:///Users/piotr/Documents/studia/SEM6/TWM/road-markings-detection/.venv/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=442'>443</a>\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    <a href='file:///Users/piotr/Documents/studia/SEM6/TWM/road-markings-detection/.venv/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=443'>444</a>\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for fold, (train_idx, val_idx) in enumerate(splits):\n",
    "\n",
    "    # print(train_idx, val_idx)\n",
    "    print(\"Fold {}\".format(fold + 1))\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(val_idx)\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "    test_loader = DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = ConvNet()\n",
    "    model = LeNet()\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.002)\n",
    "\n",
    "    history = {\"train_loss\": [], \"test_loss\": [], \"train_acc\": [], \"test_acc\": []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_correct = train_epoch(\n",
    "            model, device, train_loader, criterion, optimizer\n",
    "        )\n",
    "        test_loss, test_correct = valid_epoch(model, device, test_loader, criterion)\n",
    "\n",
    "        train_loss = train_loss / len(train_loader.sampler)\n",
    "        train_acc = train_correct / len(train_loader.sampler) * 100\n",
    "        test_loss = test_loss / len(test_loader.sampler)\n",
    "        test_acc = test_correct / len(test_loader.sampler) * 100\n",
    "\n",
    "        print(\n",
    "            \"Epoch:{}/{} AVG Training Loss:{:.3f} AVG Test Loss:{:.3f} AVG Training Acc {:.2f} % AVG Test Acc {:.2f} %\".format(\n",
    "                epoch + 1, num_epochs, train_loss, test_loss, train_acc, test_acc\n",
    "            )\n",
    "        )\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"test_loss\"].append(test_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    foldperf[\"fold{}\".format(fold + 1)] = history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "92817c6158c1804639fa34720e1636a07923a028e1d05ddaa4c830e80be69f5c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
